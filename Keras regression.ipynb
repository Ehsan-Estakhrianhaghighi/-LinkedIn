{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ef7a27f-101e-4483-9203-22ace8cf3536",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n"
     ]
    }
   ],
   "source": [
    "# Import neccessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adbfecb9-95e8-4d8b-9ebb-3af956f2ea81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the data as a panda data-frame\n",
    "\n",
    "url = 'https://cocl.us/concrete_data'\n",
    "concrete_data = pd.read_csv(url)\n",
    "\n",
    "#keep the trength column for the target and other columns for the input\n",
    "concrete_data_columns=concrete_data.columns\n",
    "predictors= concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength\n",
    "target = concrete_data['Strength']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8adb983c-bce8-45b4-a54a-81805e260a96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate  Age  \n",
       "0            1040.0           676.0   28  \n",
       "1            1055.0           676.0   28  \n",
       "2             932.0           594.0  270  \n",
       "3             932.0           594.0  365  \n",
       "4             978.4           825.5  360  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e127254-3dd4-4626-a7f4-4f60a041ce97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    79.99\n",
       "1    61.89\n",
       "2    40.27\n",
       "3    41.05\n",
       "4    44.30\n",
       "Name: Strength, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ff62b7-2fea-4d87-bea3-4a9056905b6f",
   "metadata": {},
   "source": [
    "# **Part A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd7dadc-1937-4327-a5c8-611aa6bf5f85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Creating a function for the neural network model \n",
    "def reg_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_shape=(X_train.shape[1],), activation='relu'))  # Hidden layer with 10 nodes and ReLU activation\n",
    "    model.add(Dense(1))  # Output layer with 1 node (assuming regression problem)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84066110-e34f-47fe-bfd1-ce49124fc204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 17:56:56.916955: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2023-12-18 17:56:56.923521: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394330000 Hz\n",
      "2023-12-18 17:56:56.924249: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c271dd92a0 executing computations on platform Host. Devices:\n",
      "2023-12-18 17:56:56.924297: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2023-12-18 17:56:56.951343: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309/309 [==============================] - 0s 16us/sample - loss: 118.5506\n",
      "309/309 [==============================] - 0s 24us/sample - loss: 310.1370\n",
      "309/309 [==============================] - 0s 36us/sample - loss: 380.2078\n",
      "309/309 [==============================] - 0s 31us/sample - loss: 225.2160\n",
      "309/309 [==============================] - 0s 23us/sample - loss: 392.2298\n",
      "309/309 [==============================] - 0s 22us/sample - loss: 202.9994\n",
      "309/309 [==============================] - 0s 19us/sample - loss: 1405.1617\n",
      "309/309 [==============================] - 0s 49us/sample - loss: 123.4484\n",
      "309/309 [==============================] - 0s 43us/sample - loss: 165.4925\n",
      "309/309 [==============================] - 0s 41us/sample - loss: 210.1435\n",
      "309/309 [==============================] - 0s 38us/sample - loss: 811.3516\n",
      "309/309 [==============================] - 0s 26us/sample - loss: 1052.6261\n",
      "309/309 [==============================] - 0s 19us/sample - loss: 148.8166\n",
      "309/309 [==============================] - 0s 43us/sample - loss: 1208.5051\n",
      "309/309 [==============================] - 0s 24us/sample - loss: 450.1437\n",
      "309/309 [==============================] - 0s 25us/sample - loss: 176.9918\n",
      "309/309 [==============================] - 0s 20us/sample - loss: 117.6747\n",
      "309/309 [==============================] - 0s 28us/sample - loss: 473.6228\n",
      "309/309 [==============================] - 0s 50us/sample - loss: 168.5165\n",
      "309/309 [==============================] - 0s 38us/sample - loss: 876.4949\n",
      "309/309 [==============================] - 0s 35us/sample - loss: 274.1059\n",
      "309/309 [==============================] - 0s 40us/sample - loss: 136.4965\n",
      "309/309 [==============================] - 0s 46us/sample - loss: 2033.9934\n",
      "309/309 [==============================] - 0s 52us/sample - loss: 301.9111\n",
      "309/309 [==============================] - 0s 26us/sample - loss: 168.3975\n",
      "309/309 [==============================] - 0s 43us/sample - loss: 503.7477\n",
      "309/309 [==============================] - 0s 26us/sample - loss: 268.9695\n",
      "309/309 [==============================] - 0s 24us/sample - loss: 258.3711\n",
      "309/309 [==============================] - 0s 37us/sample - loss: 184.2740\n",
      "309/309 [==============================] - 0s 49us/sample - loss: 819.9228\n",
      "309/309 [==============================] - 0s 45us/sample - loss: 135.1937\n",
      "309/309 [==============================] - 0s 28us/sample - loss: 987.4603\n",
      "309/309 [==============================] - 0s 34us/sample - loss: 120.9585\n",
      "309/309 [==============================] - 0s 44us/sample - loss: 1843.5055\n",
      "309/309 [==============================] - 0s 23us/sample - loss: 380.1695\n",
      "309/309 [==============================] - 0s 47us/sample - loss: 127.5557\n",
      "309/309 [==============================] - 0s 27us/sample - loss: 1384.4561\n",
      "309/309 [==============================] - 0s 49us/sample - loss: 234.0113\n",
      "309/309 [==============================] - 0s 46us/sample - loss: 178.8792\n",
      "309/309 [==============================] - 0s 61us/sample - loss: 145.4219\n",
      "309/309 [==============================] - 0s 30us/sample - loss: 1749.3240\n",
      "309/309 [==============================] - 0s 26us/sample - loss: 317.7562\n",
      "309/309 [==============================] - 0s 44us/sample - loss: 107.9115\n",
      "309/309 [==============================] - 0s 45us/sample - loss: 957.8189\n",
      "309/309 [==============================] - 0s 44us/sample - loss: 424.3104\n",
      "309/309 [==============================] - 0s 29us/sample - loss: 177.4810\n",
      "309/309 [==============================] - 0s 38us/sample - loss: 134.0614\n",
      "309/309 [==============================] - 0s 34us/sample - loss: 173.2882\n",
      "309/309 [==============================] - 0s 48us/sample - loss: 396.3816\n",
      "309/309 [==============================] - 0s 58us/sample - loss: 212.9889\n",
      "List_loss_A= [118.55059095956747, 310.1369680262692, 380.20779320182925, 225.21596749314983, 392.2297697098124, 202.9994211536395, 1405.1616819313815, 123.44838376338429, 165.49251173692227, 210.14349557821032, 811.3516317324345, 1052.6260969538519, 148.81664794625587, 1208.5051490759001, 450.1436684617718, 176.99183847371813, 117.67470846824276, 473.62278199427334, 168.51654907029038, 876.4948967498483, 274.10587940401246, 136.49650415710647, 2033.9933528961874, 301.9110649627389, 168.39745096249874, 503.7477286625834, 268.9695470124772, 258.37110737920966, 184.27401516121182, 819.9228434639841, 135.1936762541243, 987.4602904088289, 120.95854680823662, 1843.505536224464, 380.1695393682684, 127.55570939443643, 1384.4561202657262, 234.0113111576216, 178.87924100511668, 145.4218733704206, 1749.3240204351234, 317.7562030681129, 107.91150176409379, 957.8189444433909, 424.3104261873609, 177.48099594363117, 134.06135633153824, 173.28819704518736, 396.38156957533755, 212.98888444128931]\n",
      "List_mse_A= [118.55060030783682, 310.13695702633703, 380.2077813587945, 225.215941678557, 392.2298388869328, 202.99941501029159, 1405.1616686834939, 123.44838222050416, 165.4925130022323, 210.14349079218138, 811.3515866753671, 1052.6261054303784, 148.8166542290257, 1208.5051816109326, 450.1436814280408, 176.9918307970184, 117.67469717460608, 473.6227444609942, 168.51655595992557, 876.4949361314789, 274.10586750082786, 136.49649495967242, 2033.993384238691, 301.9111142409939, 168.3974354983844, 503.7477659468536, 268.9695676819689, 258.37109778906125, 184.27395536232902, 819.9228427824681, 135.1936795928998, 987.4602003182981, 120.95854811715698, 1843.5054848604173, 380.1695280758688, 127.55570100988577, 1384.4560756298738, 234.01130807521298, 178.87924214422637, 145.42185590067785, 1749.3239922157638, 317.7562520326921, 107.91149789919703, 957.8189401192125, 424.31041537618074, 177.48099031363222, 134.06136578643634, 173.2881991858824, 396.38155306412483, 212.98888989754147]\n",
      "List_loss_A mean= 483.1490798007014\n",
      "List_loss_A standard deviation= 489.9285805362411\n"
     ]
    }
   ],
   "source": [
    "#Randomly split the data into a training and test sets by holding 30% of the data for testing. \n",
    "# Training the model for 50 times and save the loss in a List_loss\n",
    "List_loss=[]\n",
    "List_mse=[]\n",
    "for i in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3, random_state=i)\n",
    "    model=reg_model()\n",
    "    model.fit(X_train, y_train, validation_split=0.3,  epochs=50, verbose=0)\n",
    "    loss = model.evaluate(X_test, y_test) # find the mean square error for y_test and y_pridiction\n",
    "    List_loss.append(loss)\n",
    "    #or\n",
    "    y_predict = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_predict)\n",
    "    List_mse.append(mse)\n",
    "    \n",
    "print(\"List_loss_A=\", List_loss)\n",
    "print(\"List_mse_A=\", List_mse) # This is the same as List_loss\n",
    "print(\"List_loss_A mean=\", np.array(List_loss).mean())\n",
    "print(\"List_loss_A standard deviation=\", np.array(List_loss).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf2b48a-b2d6-4ab2-ba2c-aa64955d20a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Part B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cad7397-ce57-4e71-ad91-9ce23d39ab4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for each column\n",
    "\n",
    "predictors_means = predictors.mean()\n",
    "predictors_stds = predictors.std()\n",
    "\n",
    "# Normalize each column using mean and standard deviation\n",
    "normalized_predictors = (predictors - predictors_means) / predictors_stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4045109-d4f8-4d7c-a322-d38d7348ca6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309/309 [==============================] - 0s 28us/sample - loss: 543.0295\n",
      "309/309 [==============================] - 0s 28us/sample - loss: 529.1789\n",
      "309/309 [==============================] - 0s 56us/sample - loss: 579.6266\n",
      "309/309 [==============================] - 0s 48us/sample - loss: 829.9518\n",
      "309/309 [==============================] - 0s 52us/sample - loss: 769.7714\n",
      "309/309 [==============================] - 0s 31us/sample - loss: 469.2867\n",
      "309/309 [==============================] - 0s 42us/sample - loss: 766.6932\n",
      "309/309 [==============================] - 0s 40us/sample - loss: 823.8795\n",
      "309/309 [==============================] - 0s 48us/sample - loss: 676.9201\n",
      "309/309 [==============================] - 0s 68us/sample - loss: 498.3110\n",
      "309/309 [==============================] - 0s 48us/sample - loss: 806.9607\n",
      "309/309 [==============================] - 0s 45us/sample - loss: 656.7293\n",
      "309/309 [==============================] - 0s 50us/sample - loss: 517.1478\n",
      "309/309 [==============================] - 0s 33us/sample - loss: 845.8288\n",
      "309/309 [==============================] - 0s 56us/sample - loss: 613.2409\n",
      "309/309 [==============================] - 0s 56us/sample - loss: 612.1569\n",
      "309/309 [==============================] - 0s 44us/sample - loss: 606.7680\n",
      "309/309 [==============================] - 0s 36us/sample - loss: 624.7430\n",
      "309/309 [==============================] - 0s 38us/sample - loss: 665.5523\n",
      "309/309 [==============================] - 0s 55us/sample - loss: 561.3706\n",
      "309/309 [==============================] - 0s 44us/sample - loss: 847.6790\n",
      "309/309 [==============================] - 0s 36us/sample - loss: 659.2300\n",
      "309/309 [==============================] - 0s 50us/sample - loss: 559.5724\n",
      "309/309 [==============================] - 0s 65us/sample - loss: 662.0215\n",
      "309/309 [==============================] - 0s 51us/sample - loss: 707.8414\n",
      "309/309 [==============================] - 0s 47us/sample - loss: 698.1675\n",
      "309/309 [==============================] - 0s 48us/sample - loss: 881.8669\n",
      "309/309 [==============================] - 0s 36us/sample - loss: 409.6567\n",
      "309/309 [==============================] - 0s 67us/sample - loss: 895.2640\n",
      "309/309 [==============================] - 0s 50us/sample - loss: 577.0054\n",
      "309/309 [==============================] - 0s 125us/sample - loss: 593.1752\n",
      "309/309 [==============================] - 0s 82us/sample - loss: 771.0473\n",
      "309/309 [==============================] - 0s 48us/sample - loss: 684.5505\n",
      "309/309 [==============================] - 0s 42us/sample - loss: 790.3569\n",
      "309/309 [==============================] - 0s 44us/sample - loss: 701.9649\n",
      "309/309 [==============================] - 0s 41us/sample - loss: 1003.3143\n",
      "309/309 [==============================] - 0s 50us/sample - loss: 684.1979\n",
      "309/309 [==============================] - 0s 72us/sample - loss: 549.6593\n",
      "309/309 [==============================] - 0s 38us/sample - loss: 696.0739\n",
      "309/309 [==============================] - 0s 46us/sample - loss: 452.0391\n",
      "309/309 [==============================] - 0s 46us/sample - loss: 732.2224\n",
      "309/309 [==============================] - 0s 65us/sample - loss: 649.8188\n",
      "309/309 [==============================] - 0s 55us/sample - loss: 644.4312\n",
      "309/309 [==============================] - 0s 63us/sample - loss: 679.5687\n",
      "309/309 [==============================] - 0s 66us/sample - loss: 584.0173\n",
      "309/309 [==============================] - 0s 86us/sample - loss: 442.8750\n",
      "309/309 [==============================] - 0s 54us/sample - loss: 474.9491\n",
      "309/309 [==============================] - 0s 65us/sample - loss: 464.4360\n",
      "309/309 [==============================] - 0s 54us/sample - loss: 640.5432\n",
      "309/309 [==============================] - 0s 53us/sample - loss: 564.7706\n",
      "List_loss_B= [543.0295244235436, 529.1788642167273, 579.6265515571273, 829.9517520052715, 769.7714409195489, 469.28667798088594, 766.6932479710255, 823.8794571466045, 676.9201099185882, 498.31100197208735, 806.9607334569048, 656.7293319949055, 517.1477968283841, 845.8287878931533, 613.2408648740898, 612.1569200040453, 606.7680418144153, 624.7430216471354, 665.5523405105937, 561.3706046786509, 847.6790113726865, 659.2299670370651, 559.5723960901155, 662.0215093618843, 707.84142849129, 698.1675247143002, 881.8668605964932, 409.65672642976335, 895.263969347315, 577.0053714887996, 593.1751705033879, 771.0473460965943, 684.5504831851107, 790.3569181868173, 701.96491031585, 1003.3142617234906, 684.1979318760746, 549.6593402751441, 696.0739283885771, 452.0391035851537, 732.2223726044195, 649.8187718067354, 644.4312155516787, 679.5687142282628, 584.0173389224944, 442.87502217215626, 474.94905174280063, 464.4359719483212, 640.543240050282, 564.7705579837935]\n",
      "List_mse_B= [543.0295293626497, 529.1788652431944, 579.6265410879928, 829.9517511977659, 769.7714244239057, 469.28668161428345, 766.6932431975746, 823.8794517769786, 676.9201032760616, 498.3109973280377, 806.9607234867103, 656.72931627024, 517.1478110272681, 845.8287874164675, 613.2408790064515, 612.1569154719882, 606.7680590721825, 624.7430209612701, 665.5523192990975, 561.3706034952631, 847.678990417313, 659.2299696409656, 559.5723730305109, 662.0214977877017, 707.8414250254071, 698.1675225067817, 881.8668520229501, 409.6567171941198, 895.2639835537268, 577.0053598864544, 593.175187683437, 771.0473399530351, 684.5504864569356, 790.3569035510397, 701.964892627464, 1003.3142643247589, 684.1979209545407, 549.6593381026565, 696.0739409881977, 452.0391068235546, 732.2223752738447, 649.8187685407661, 644.4311988205028, 679.5687126373438, 584.0173200331769, 442.8750139534642, 474.9490524190683, 464.4359742167764, 640.54324024151, 564.770546062707]\n",
      "List_loss_B mean= 653.989269757811\n",
      "List_loss_B standard deviation= 130.19996209863\n"
     ]
    }
   ],
   "source": [
    "#Randomly split the data into a training and test sets by holding 30% of the data for testing. \n",
    "# Training the model for 50 times and save the loss in a List_loss\n",
    "List_loss=[]\n",
    "List_mse=[]\n",
    "for i in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(normalized_predictors, target, test_size=0.3, random_state=i)\n",
    "    model=reg_model()\n",
    "    model.fit(X_train, y_train, validation_split=0.3, epochs=50, verbose=0)\n",
    "    loss = model.evaluate(X_test, y_test)  #find the mean square error for y_test and y_pridiction\n",
    "    List_loss.append(loss)\n",
    "    #or\n",
    "    y_predict = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_predict)\n",
    "    List_mse.append(mse)\n",
    "    \n",
    "print(\"List_loss_B=\", List_loss)\n",
    "print(\"List_mse_B=\", List_mse) # This is the same as List_loss\n",
    "print(\"List_loss_B mean=\", np.array(List_loss).mean())\n",
    "print(\"List_loss_B standard deviation=\", np.array(List_loss).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26d6474-f43e-445e-9a2e-0268e68e2745",
   "metadata": {},
   "source": [
    "__An improvement compare to A__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978eb668-db58-4009-9c0e-87666844bdd0",
   "metadata": {},
   "source": [
    "# **Part C**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4c634d4-3220-42cc-adbe-e2638edc9d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for each column\n",
    "\n",
    "predictors_means = predictors.mean()\n",
    "predictors_stds = predictors.std()\n",
    "\n",
    "# Normalize each column using mean and standard deviation\n",
    "normalized_predictors = (predictors - predictors_means) / predictors_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc1c2681-cc18-4e58-a19d-9899f032355c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309/309 [==============================] - 0s 58us/sample - loss: 218.8046\n",
      "309/309 [==============================] - 0s 81us/sample - loss: 190.7583\n",
      "309/309 [==============================] - 0s 51us/sample - loss: 166.2923\n",
      "309/309 [==============================] - 0s 59us/sample - loss: 224.2688\n",
      "309/309 [==============================] - 0s 63us/sample - loss: 238.7174\n",
      "309/309 [==============================] - 0s 44us/sample - loss: 187.7473\n",
      "309/309 [==============================] - 0s 64us/sample - loss: 186.2456\n",
      "309/309 [==============================] - 0s 61us/sample - loss: 190.8784\n",
      "309/309 [==============================] - 0s 49us/sample - loss: 202.5159\n",
      "309/309 [==============================] - 0s 63us/sample - loss: 196.7088\n",
      "309/309 [==============================] - 0s 50us/sample - loss: 173.9122\n",
      "309/309 [==============================] - 0s 54us/sample - loss: 194.8169\n",
      "309/309 [==============================] - 0s 45us/sample - loss: 237.3745\n",
      "309/309 [==============================] - 0s 59us/sample - loss: 238.5106\n",
      "309/309 [==============================] - 0s 53us/sample - loss: 197.6243\n",
      "309/309 [==============================] - 0s 61us/sample - loss: 244.0467\n",
      "309/309 [==============================] - 0s 66us/sample - loss: 251.0957\n",
      "309/309 [==============================] - 0s 57us/sample - loss: 206.7828\n",
      "309/309 [==============================] - 0s 184us/sample - loss: 171.0407\n",
      "309/309 [==============================] - 0s 55us/sample - loss: 177.9064\n",
      "309/309 [==============================] - 0s 58us/sample - loss: 178.0327\n",
      "309/309 [==============================] - 0s 64us/sample - loss: 189.6564\n",
      "309/309 [==============================] - 0s 51us/sample - loss: 263.3127\n",
      "309/309 [==============================] - 0s 64us/sample - loss: 184.4704\n",
      "309/309 [==============================] - 0s 65us/sample - loss: 223.3745\n",
      "309/309 [==============================] - 0s 74us/sample - loss: 182.4510\n",
      "309/309 [==============================] - 0s 61us/sample - loss: 185.8700\n",
      "309/309 [==============================] - 0s 53us/sample - loss: 215.8077\n",
      "309/309 [==============================] - 0s 61us/sample - loss: 211.0422\n",
      "309/309 [==============================] - 0s 65us/sample - loss: 246.2576\n",
      "309/309 [==============================] - 0s 67us/sample - loss: 199.0004\n",
      "309/309 [==============================] - 0s 117us/sample - loss: 248.5477\n",
      "309/309 [==============================] - 0s 76us/sample - loss: 195.0823\n",
      "309/309 [==============================] - 0s 95us/sample - loss: 210.3107\n",
      "309/309 [==============================] - 0s 80us/sample - loss: 167.0817\n",
      "309/309 [==============================] - 0s 74us/sample - loss: 200.6570\n",
      "309/309 [==============================] - 0s 61us/sample - loss: 191.4256\n",
      "309/309 [==============================] - 0s 66us/sample - loss: 291.6523\n",
      "309/309 [==============================] - 0s 66us/sample - loss: 189.5522\n",
      "309/309 [==============================] - 0s 65us/sample - loss: 183.5739\n",
      "309/309 [==============================] - 0s 56us/sample - loss: 199.5534\n",
      "309/309 [==============================] - 0s 70us/sample - loss: 193.5706\n",
      "309/309 [==============================] - 0s 88us/sample - loss: 178.5844\n",
      "309/309 [==============================] - 0s 84us/sample - loss: 186.9738\n",
      "309/309 [==============================] - 0s 141us/sample - loss: 164.6193\n",
      "309/309 [==============================] - 0s 75us/sample - loss: 230.6281\n",
      "309/309 [==============================] - 0s 69us/sample - loss: 190.3288\n",
      "309/309 [==============================] - 0s 73us/sample - loss: 184.1073\n",
      "309/309 [==============================] - 0s 65us/sample - loss: 229.1377\n",
      "309/309 [==============================] - 0s 63us/sample - loss: 173.6378\n",
      "List_loss_C= [218.80458760184376, 190.75834394041388, 166.29228882033462, 224.26879912441217, 238.7174423859729, 187.74734427936642, 186.24557672889486, 190.87844127667375, 202.51586593084735, 196.70882003438513, 173.91217934815242, 194.81690045699332, 237.37450529919474, 238.51056293376442, 197.62433720560907, 244.04672611569896, 251.09574396324774, 206.7827717308859, 171.04065509289984, 177.90644540138615, 178.032660475055, 189.65635478689447, 263.31270443814475, 184.47035562953516, 223.37449048483643, 182.4510094108705, 185.86997027844671, 215.80765437153937, 211.04224546364597, 246.2576175430446, 199.00038072900864, 248.54769467844545, 195.08225099631497, 210.31065279766193, 167.0816974825072, 200.65695289192075, 191.42559809515006, 291.65232775049304, 189.5522479208542, 183.57386290911333, 199.5533972681533, 193.57063535424885, 178.58444944708864, 186.9737655491505, 164.6193136320145, 230.62812429878704, 190.3287935226095, 184.1072804966405, 229.13774252092182, 173.63784088826102]\n",
      "List_mse_C= [218.804593397927, 190.75835112656307, 166.29229968319237, 224.26880374963594, 238.7174515256193, 187.7473391479356, 186.2455748023668, 190.87844225696568, 202.5158593729704, 196.70882805293354, 173.91217885152378, 194.81690877218125, 237.37450857771208, 238.51055799070795, 197.62434368374971, 244.04672935582425, 251.09574980993227, 206.78277582783667, 171.04065206803512, 177.9064442290891, 178.03265912015058, 189.65636036350983, 263.31271013050105, 184.47036282474025, 223.37448845439604, 182.4510202968971, 185.86996356684668, 215.80765795366443, 211.04223710846443, 246.25762243876238, 199.00038390223892, 248.54769848171586, 195.08225872855976, 210.3106612273173, 167.08169265464466, 200.656958480921, 191.42559612113934, 291.6523355027872, 189.55224501323517, 183.57386354408544, 199.55339499990487, 193.57063699607042, 178.58445047198447, 186.97376815865925, 164.61931090178214, 230.62812975174015, 190.3287914819719, 184.10729138449125, 229.13774472261775, 173.63784347078152]\n",
      "List_loss_C mean= 203.68696819564667\n",
      "List_loss_C standard deviation= 27.755920728031697\n"
     ]
    }
   ],
   "source": [
    "# Training the model for 50 times and save the loss in a List_loss\n",
    "List_loss=[]\n",
    "List_mse=[]\n",
    "for i in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(normalized_predictors, target, test_size=0.3, random_state=i)\n",
    "    model=reg_model()\n",
    "    model_history= model.fit(X_train, y_train,validation_split=0.3, epochs=100, verbose=0)\n",
    "    loss = model.evaluate(X_test, y_test)\n",
    "    List_loss.append(loss)\n",
    "    #or\n",
    "    y_predict = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_predict)\n",
    "    List_mse.append(mse)\n",
    "    \n",
    "print(\"List_loss_C=\", List_loss)\n",
    "print(\"List_mse_C=\", List_mse) # This is the same as List_loss\n",
    "print(\"List_loss_C mean=\", np.array(List_loss).mean())\n",
    "print(\"List_loss_C standard deviation=\", np.array(List_loss).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0e239d-608d-413d-9d8f-5e11d313c568",
   "metadata": {},
   "source": [
    "__An improvement compare to B__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ead8e5-3c69-4bf3-905a-4cc8a736cd4d",
   "metadata": {},
   "source": [
    "# **Part D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e593d06-1a4e-4d1e-b817-2044487c42f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for each column\n",
    "\n",
    "predictors_means = predictors.mean()\n",
    "predictors_stds = predictors.std()\n",
    "\n",
    "# Normalize each column using mean and standard deviation\n",
    "normalized_predictors = (predictors - predictors_means) / predictors_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "631e2e61-8b51-4cdc-9437-74d7601a7032",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a function for the neural network model \n",
    "def reg_model_new():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_shape=(X_train.shape[1],), activation='relu'))  # Hidden layer with 10 nodes and ReLU activation\n",
    "    model.add(Dense(10, activation='relu')) \n",
    "    model.add(Dense(10, activation='relu')) \n",
    "    model.add(Dense(1))  # Output layer with 1 node (assuming regression problem)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb095f9a-6e32-4c1b-8e75-ae5c05b9f8da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309/309 [==============================] - 0s 85us/sample - loss: 126.5283\n",
      "309/309 [==============================] - 0s 63us/sample - loss: 166.1584\n",
      "309/309 [==============================] - 0s 73us/sample - loss: 139.4290\n",
      "309/309 [==============================] - 0s 76us/sample - loss: 164.2871\n",
      "309/309 [==============================] - 0s 74us/sample - loss: 165.8540\n",
      "309/309 [==============================] - 0s 67us/sample - loss: 149.7108\n",
      "309/309 [==============================] - 0s 74us/sample - loss: 187.1466\n",
      "309/309 [==============================] - 0s 67us/sample - loss: 145.6961\n",
      "309/309 [==============================] - 0s 83us/sample - loss: 148.3597\n",
      "309/309 [==============================] - 0s 66us/sample - loss: 137.5145\n",
      "309/309 [==============================] - 0s 92us/sample - loss: 137.2873\n",
      "309/309 [==============================] - 0s 82us/sample - loss: 144.8660\n",
      "309/309 [==============================] - 0s 84us/sample - loss: 171.2936\n",
      "309/309 [==============================] - 0s 86us/sample - loss: 173.9666\n",
      "309/309 [==============================] - 0s 141us/sample - loss: 155.4601\n",
      "309/309 [==============================] - 0s 82us/sample - loss: 147.0543\n",
      "309/309 [==============================] - 0s 97us/sample - loss: 125.5197\n",
      "309/309 [==============================] - 0s 70us/sample - loss: 123.2226\n",
      "309/309 [==============================] - 0s 198us/sample - loss: 135.6471\n",
      "309/309 [==============================] - 0s 128us/sample - loss: 128.2820\n",
      "309/309 [==============================] - 0s 87us/sample - loss: 134.0573\n",
      "309/309 [==============================] - 0s 90us/sample - loss: 139.4708\n",
      "309/309 [==============================] - 0s 69us/sample - loss: 149.7164\n",
      "309/309 [==============================] - 0s 94us/sample - loss: 134.9364\n",
      "309/309 [==============================] - 0s 90us/sample - loss: 141.8392\n",
      "309/309 [==============================] - 0s 92us/sample - loss: 131.9825\n",
      "309/309 [==============================] - 0s 74us/sample - loss: 174.0504\n",
      "309/309 [==============================] - 0s 96us/sample - loss: 154.3337\n",
      "309/309 [==============================] - 0s 104us/sample - loss: 151.2048\n",
      "309/309 [==============================] - 0s 86us/sample - loss: 147.4825\n",
      "309/309 [==============================] - 0s 84us/sample - loss: 181.8514\n",
      "309/309 [==============================] - 0s 85us/sample - loss: 143.7994\n",
      "309/309 [==============================] - 0s 92us/sample - loss: 151.6627\n",
      "309/309 [==============================] - 0s 87us/sample - loss: 142.4312\n",
      "309/309 [==============================] - 0s 106us/sample - loss: 146.1321\n",
      "309/309 [==============================] - 0s 77us/sample - loss: 168.2933\n",
      "309/309 [==============================] - 0s 91us/sample - loss: 158.4251\n",
      "309/309 [==============================] - 0s 79us/sample - loss: 133.1205\n",
      "309/309 [==============================] - 0s 126us/sample - loss: 156.5396\n",
      "309/309 [==============================] - 0s 100us/sample - loss: 157.9827\n",
      "309/309 [==============================] - 0s 100us/sample - loss: 137.7644\n",
      "309/309 [==============================] - 0s 93us/sample - loss: 151.2201\n",
      "309/309 [==============================] - 0s 101us/sample - loss: 164.2153\n",
      "309/309 [==============================] - 0s 101us/sample - loss: 154.9985\n",
      "309/309 [==============================] - 0s 94us/sample - loss: 148.7643\n",
      "309/309 [==============================] - 0s 101us/sample - loss: 163.4357\n",
      "309/309 [==============================] - 0s 164us/sample - loss: 139.9986\n",
      "309/309 [==============================] - 0s 88us/sample - loss: 176.6985\n",
      "309/309 [==============================] - 0s 88us/sample - loss: 145.8816\n",
      "309/309 [==============================] - 0s 105us/sample - loss: 160.9145\n",
      "List_loss_D= [126.52827702988313, 166.1583787245272, 139.42904159397756, 164.28714199127887, 165.85402830056003, 149.7107846652034, 187.1465740018678, 145.6960800812854, 148.35970723667577, 137.51451599713667, 137.28727322180293, 144.86604042423582, 171.29361633498306, 173.96661653487814, 155.46005283590273, 147.0543431155504, 125.51971035559201, 123.22256825271162, 135.64712786134393, 128.28199970529303, 134.0573090488471, 139.47082914580804, 149.7164263185174, 134.93644119312077, 141.83920653506775, 131.982525526127, 174.05037953398374, 154.33366315033058, 151.204826873483, 147.4825432539758, 181.8514105046837, 143.79943641489763, 151.66266422210003, 142.43116226937008, 146.13210254656843, 168.2933095790036, 158.42512112219356, 133.12052142195716, 156.53963460582744, 157.98265549743059, 137.764439604429, 151.22014561131547, 164.21534911714326, 154.99850728056578, 148.76427911554725, 163.43572220293063, 139.99859379641833, 176.6984951673588, 145.88158315516597, 160.91450712827418]\n",
      "List_mse_D= [126.52827717931409, 166.1583849481324, 139.42903822599055, 164.28713225277596, 165.85403389963116, 149.71078128105808, 187.14656783963756, 145.6960804067322, 148.3597062227892, 137.51451401889224, 137.28726891989493, 144.86603567986447, 171.29361499293498, 173.96661001752264, 155.46004828547646, 147.05434026880397, 125.51970998558294, 123.22256765546038, 135.6471274133266, 128.28199511694794, 134.0573047109428, 139.47082895799147, 149.71642213522705, 134.93643524439702, 141.83920761233642, 131.98251751561125, 174.05038004216675, 154.33366082053442, 151.20482849320447, 147.4825324039289, 181.85141050404113, 143.79942964213652, 151.66266678514808, 142.43115864534755, 146.1320970883215, 168.29331602393134, 158.42512412091946, 133.12052683114436, 156.53964031947945, 157.98266320387978, 137.76444068363384, 151.2201426933399, 164.2153514578062, 154.99850371671636, 148.76428271390978, 163.43572399521744, 139.99858727760648, 176.6984899781929, 145.88157857490262, 160.9145023541618]\n",
      "List_loss_D mean= 150.32975338414263\n",
      "List_loss_D standard deviation= 14.967696262477547\n"
     ]
    }
   ],
   "source": [
    "# Training the model for 50 times and save the loss in a List_loss\n",
    "List_loss=[]\n",
    "List_mse=[]\n",
    "for i in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(normalized_predictors, target, test_size=0.3, random_state=i)\n",
    "    model=reg_model_new()\n",
    "    model_history= model.fit(X_train, y_train, epochs=50, validation_split=0.3, verbose=0)\n",
    "    loss = model.evaluate(X_test, y_test)\n",
    "    List_loss.append(loss)\n",
    "    #or\n",
    "    y_predict = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_predict)\n",
    "    List_mse.append(mse)\n",
    "    \n",
    "print(\"List_loss_D=\", List_loss)\n",
    "print(\"List_mse_D=\", List_mse) # This is the same as List_loss\n",
    "print(\"List_loss_D mean=\", np.array(List_loss).mean())\n",
    "print(\"List_loss_D standard deviation=\", np.array(List_loss).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201022b4-e0f1-4f7e-b54e-cd70f1387936",
   "metadata": {},
   "source": [
    "__Considerable improvement compare to B__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
